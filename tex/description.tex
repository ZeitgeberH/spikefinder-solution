\documentclass{report}
\begin{document}

The submitted solution is generated by a convolutional neural network. Here 
we describe some basic properties of the model and discuss possible extensions
and improvements for future algorithms.

\section*{Methods}

The inputs to the network are the calcium signal and an index vector denoting
from which dataset the inputs are from. The network consists of 8 convolutional
layers and one recurrent layer (LSTM). The learning is achieved by minimizing
the Pearson correlation between the model output and the ground-truth spiking
data. The used optimization method is `adam' \cite{Adam}.

The model is depicted in Figure 1. 
The first layer consists of 10 units. Each unit uses a kernel with a width of
300 datapoints (3 seconds?) that is convolved (actually correlated) with the
input calcium signal. The learned kernels catch a basic repertoire of dynamics
as shown in Figure 2. The output of this layer is modulated through a `tanh'
activation function. This layer is followed by multiple convolutional layers
with smaller kernel widths (100 and 50 ms) and with rectifying activation
functions (`relu'). The dataset index input is added concatenated to the output
of the second layer and feed into the input of the third layer. Moreover, a
recurrent neural group (a LSTM layer) is fed by the input of the third layer,
and its output is added to the input to the forth layers. The following 4 layers
have decreasing size, as the last layer consists of a single unit.

To distribute the amount of information that different units are caring, a
dropout is applied at the output of the first 4 convolutional layers. An
attempt to implement a batch normalization failed.

\section*{Discussion}
using strategies to optimise the hyper-paramters of the model ( e.g., spearmint) might further 
increase the performance. More epochs? here we used 50 epochs.



\end{document}
